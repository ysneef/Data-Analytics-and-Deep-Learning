{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2KD86yFiI8Kk/9cXxv4d+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ysneef/Data-Analytics-and-Deep-Learning/blob/main/2174802010854-VoHuynhNhuY-Lab07/lab7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LAB 7:\n",
        "# PHÂN TÍCH DỮ LIỆU DẠNG VĂN BẢN VỚI NLTK"
      ],
      "metadata": {
        "id": "0yGNMzt1n-xY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**1. Giới thiệu về thư viện NLTK:**\n",
        "\n"
      ],
      "metadata": {
        "id": "_F1L8n0ToD8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import thư viện NLTK và download công cụng NLTK"
      ],
      "metadata": {
        "id": "BwukG2qboKPf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ2gXTgImzN8",
        "outputId": "b3712cc8-5251-4372-dd41-36e82645767a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> gutenberg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Downloading package gutenberg to /root/nltk_data...\n",
            "      Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download_shell()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Đối với những lần sau, nếu bạn đã biết tên của gói bạn muốn tải xuống, chỉ cần nhập lệnh\n",
        "nltk.download() với tên gói là tham số đầu vào. Thao tác này sẽ không mở công cụ NLTK\n",
        "Downloader, nhưng sẽ trực tiếp tải xuống gói yêu cầu. Vì vậy, thao tác trước đó tương đương với\n",
        "việc viết lệnh sau:"
      ],
      "metadata": {
        "id": "O8zgs5-LrJKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('gutenberg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGyVPiGfrJ4J",
        "outputId": "6449db25-f8e3-4091-9489-b63ca4a3476d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpLbXTTnnjd6",
        "outputId": "a003944a-2115-461e-bb31-b43dd205a5d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sau khi hoàn tất, bạn có xem tên các tập tin có trong gói 'gutenberg' bằng lệnh fileids()"
      ],
      "metadata": {
        "id": "8_Xb-hXCrEN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gb = nltk.corpus.gutenberg\n",
        "print(\"Gutenberg files : \", gb.fileids())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6TJiV48naOo",
        "outputId": "af8c1c14-9a98-4c4e-fa68-4c3d89e6037b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gutenberg files :  ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Để truy cập nội dung bên trong của một trong các tập tin này, trước tiên bạn chọn một tập tin,\n",
        "chẳng hạn Macbeth của Shakespeare (shakespeare-macbeth.txt), sử dụng hàm words (), rồi gán\n",
        "cho một biến nào đó."
      ],
      "metadata": {
        "id": "Ld-Aw8ANrY2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "macbeth = nltk.corpus.gutenberg.words('shakespeare-macbeth.txt')"
      ],
      "metadata": {
        "id": "Jz7z97h6nfgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nếu bạn muốn biết chiều dài của văn bản trên (bao nhiêu từ), bạn dùng hàm len()\n"
      ],
      "metadata": {
        "id": "XMlTIErgrbfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(macbeth)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDqi-UXinzp5",
        "outputId": "bdcb172c-ddf9-4c08-d55f-5051c476e22e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23140"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nếu bạn muốn hiển thị 10 từ đầu tiên của tập tin\n"
      ],
      "metadata": {
        "id": "KU-B332Xrdkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "macbeth [:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyaIUVkRn2oL",
        "outputId": "1f81f3a3-71c1-4a25-963c-3f9de97fc962"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[',\n",
              " 'The',\n",
              " 'Tragedie',\n",
              " 'of',\n",
              " 'Macbeth',\n",
              " 'by',\n",
              " 'William',\n",
              " 'Shakespeare',\n",
              " '1603',\n",
              " ']']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ta muốn trích 5 câu đầu tiên của tập tin (một câu được kẹp trong cặp ngoặc vuông), ta dùng hàm\n",
        "sent()\n"
      ],
      "metadata": {
        "id": "lw8bv_fPrnmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "macbeth_sents = nltk.corpus.gutenberg.sents('shakespeare-macbeth.txt')\n",
        "macbeth_sents[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3ir18qmn4pW",
        "outputId": "858349e8-ed85-4752-d9ce-fe5990e8c052"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['[',\n",
              "  'The',\n",
              "  'Tragedie',\n",
              "  'of',\n",
              "  'Macbeth',\n",
              "  'by',\n",
              "  'William',\n",
              "  'Shakespeare',\n",
              "  '1603',\n",
              "  ']'],\n",
              " ['Actus', 'Primus', '.'],\n",
              " ['Scoena', 'Prima', '.'],\n",
              " ['Thunder', 'and', 'Lightning', '.'],\n",
              " ['Enter', 'three', 'Witches', '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Tìm 1 từ với NLTK:**"
      ],
      "metadata": {
        "id": "jMfBsuSmohQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tìm từ ‘Stage’ xuất hiện trong văn bản text"
      ],
      "metadata": {
        "id": "XgdB_yuIojn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = nltk.Text(macbeth)\n",
        "text.concordance('Stage')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVErHIsEohpb",
        "outputId": "6b3b1a5f-9f06-433a-f397-a890a0c5aa2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 3 of 3 matches:\n",
            "nts with Dishes and Seruice ouer the Stage . Then enter Macbeth Macb . If it we\n",
            "with mans Act , Threatens his bloody Stage : byth ' Clock ' tis Day , And yet d\n",
            " struts and frets his houre vpon the Stage , And then is heard no more . It is \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tìm từ xuất hiện trước và sau từ ‘Stage’"
      ],
      "metadata": {
        "id": "VH9NDiahoo0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text.common_contexts(['Stage'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-32zS5Iaop3y",
        "outputId": "0fa8867e-2dd0-41ba-e098-e298647633cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the_. bloody_: the_,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tìm từ tương tự từ ‘Stage’"
      ],
      "metadata": {
        "id": "X7JN1GZkor-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text.similar('Stage')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x86u-QU9otMV",
        "outputId": "3b95d55d-a4a8-4500-f1f4-900d29a9b4d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "day time face warre ayre king bleeding man reuolt serieant like\n",
            "knowledge broyle shew head spring heeles hare thane skie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Phân tích tần số của các từ**"
      ],
      "metadata": {
        "id": "LIHo_YRfo9Ya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Muốn xem 10 từ thông dụng nhất trong văn bản xuất hiện bao nhiêu lần, dùng lệnh\n",
        "most_common()"
      ],
      "metadata": {
        "id": "JXhdGVjopBFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fd = nltk.FreqDist(macbeth)\n",
        "fd.most_common(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5P6_wu-7o-QS",
        "outputId": "e89810f1-4122-4ae0-e83e-2c372ebcb839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 1962),\n",
              " ('.', 1235),\n",
              " (\"'\", 637),\n",
              " ('the', 531),\n",
              " (':', 477),\n",
              " ('and', 376),\n",
              " ('I', 333),\n",
              " ('of', 315),\n",
              " ('to', 311),\n",
              " ('?', 241)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Muốn download stopword"
      ],
      "metadata": {
        "id": "hLFY83sdpGQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piUasb84pGrp",
        "outputId": "0ff7d2d2-62af-4252-809f-ebedc388ce92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Muốn xem các stopword trong tiếng Anh, dùng lệnh"
      ],
      "metadata": {
        "id": "uDKahQvNpJXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sw = set(nltk.corpus.stopwords.words('english'))\n",
        "print(len(sw))\n",
        "list(sw)[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkFnWYewpKID",
        "outputId": "fe5c2a39-928a-4506-c0af-655da0b66894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "198\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['up',\n",
              " 'couldn',\n",
              " \"don't\",\n",
              " 'will',\n",
              " 'because',\n",
              " 'if',\n",
              " 'don',\n",
              " \"we'd\",\n",
              " 'after',\n",
              " 'but']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Có 179 stopword trong từ vựng tiếng Anh. Ta sẽ loại bỏ các từ stopword trong biến macbeth"
      ],
      "metadata": {
        "id": "EXImCHQqpL6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "macbeth_filtered = [w for w in macbeth if w.lower() not in sw]\n",
        "len(macbeth_filtered)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgl4u_TBpNoh",
        "outputId": "825fb2c8-a446-4ae0-eb9a-a030c7d761c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14946"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fd = nltk.FreqDist(macbeth_filtered)\n",
        "fd.most_common(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdq-wLxZpTLI",
        "outputId": "3afc5eac-363b-463d-f1ee-13ef0e991a7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 1962),\n",
              " ('.', 1235),\n",
              " (\"'\", 637),\n",
              " (':', 477),\n",
              " ('?', 241),\n",
              " ('Macb', 137),\n",
              " ('haue', 117),\n",
              " ('-', 100),\n",
              " ('Enter', 80),\n",
              " ('thou', 63)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "loại bỏ các dấu câu theo lệnh sau"
      ],
      "metadata": {
        "id": "9X3s0vYFpQQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "punctuation = set(string.punctuation)\n",
        "macbeth_filtered2 = [w.lower() for w in macbeth if w.lower() not in sw and w.lower() not in punctuation]\n",
        "fd = nltk.FreqDist(macbeth_filtered2)\n",
        "fd.most_common(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IggiyrpgpXjR",
        "outputId": "047af689-338a-4d1e-b486-be2a184e433c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('macb', 137),\n",
              " ('haue', 122),\n",
              " ('thou', 90),\n",
              " ('enter', 81),\n",
              " ('shall', 68),\n",
              " ('macbeth', 62),\n",
              " ('vpon', 62),\n",
              " ('thee', 61),\n",
              " ('macd', 58),\n",
              " ('vs', 57)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Lựa chọn các từ trong văn bản**"
      ],
      "metadata": {
        "id": "SlAMhU1Xpkkv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rút trích các từ có độ dài lớn nhất, ví dụ các từ có độ dài lớn hơn 12 ký tự , dùng lệnh sau:"
      ],
      "metadata": {
        "id": "Vp81LgOTp9YB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "long_words = [w for w in macbeth if len(w)> 12]\n",
        "sorted(long_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNf4ucHOpk2G",
        "outputId": "7c870dd4-ee2c-49ee-c90e-c6570bb2e11e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Assassination',\n",
              " 'Chamberlaines',\n",
              " 'Distinguishes',\n",
              " 'Gallowgrosses',\n",
              " 'Metaphysicall',\n",
              " 'Northumberland',\n",
              " 'Voluptuousnesse',\n",
              " 'commendations',\n",
              " 'multitudinous',\n",
              " 'supernaturall',\n",
              " 'vnaccompanied']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rút trích các từ có chứa chuỗi ‘ious’\n"
      ],
      "metadata": {
        "id": "VOxopLAqqAXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ious_words = [w for w in macbeth if 'ious' in w]\n",
        "ious_words = set(ious_words)\n",
        "sorted(ious_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySthhK-DqA3X",
        "outputId": "33d99a15-16a0-40a2-d5cf-1d00320a3770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Auaricious',\n",
              " 'Gracious',\n",
              " 'Industrious',\n",
              " 'Iudicious',\n",
              " 'Luxurious',\n",
              " 'Malicious',\n",
              " 'Obliuious',\n",
              " 'Pious',\n",
              " 'Rebellious',\n",
              " 'compunctious',\n",
              " 'furious',\n",
              " 'gracious',\n",
              " 'pernicious',\n",
              " 'pernitious',\n",
              " 'pious',\n",
              " 'precious',\n",
              " 'rebellious',\n",
              " 'sacrilegious',\n",
              " 'serious',\n",
              " 'spacious',\n",
              " 'tedious']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Bigrams và collocations**\n"
      ],
      "metadata": {
        "id": "BcrVDbylqCzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lọc các bigram sau khi đã loại các stopword và các dấu câu, dùng lệnh sau:"
      ],
      "metadata": {
        "id": "0gV7JzV7qGl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bgrms = nltk.FreqDist(nltk.bigrams(macbeth_filtered2))\n",
        "bgrms.most_common(15)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmE3AK-UqEZj",
        "outputId": "752ffc6c-ff6d-4fb9-905a-0c5cbb4a1d4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('enter', 'macbeth'), 16),\n",
              " (('exeunt', 'scena'), 15),\n",
              " (('thane', 'cawdor'), 13),\n",
              " (('knock', 'knock'), 10),\n",
              " (('st', 'thou'), 9),\n",
              " (('thou', 'art'), 9),\n",
              " (('lord', 'macb'), 9),\n",
              " (('haue', 'done'), 8),\n",
              " (('macb', 'haue'), 8),\n",
              " (('good', 'lord'), 8),\n",
              " (('let', 'vs'), 7),\n",
              " (('enter', 'lady'), 7),\n",
              " (('wee', 'l'), 7),\n",
              " (('would', 'st'), 6),\n",
              " (('macbeth', 'macb'), 6)]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ngoài bigram ra, còn có trigram, sự kết hợp của 3 từ, ta dùng lệnh trigrams()\n"
      ],
      "metadata": {
        "id": "ZjKpQx07qLI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tgrms = nltk.FreqDist(nltk.trigrams (macbeth_filtered2))\n",
        "tgrms.most_common(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSgcFCgrqLZJ",
        "outputId": "38ad47dc-179b-4b07-a980-ee31976bfcd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('knock', 'knock', 'knock'), 6),\n",
              " (('enter', 'macbeth', 'macb'), 5),\n",
              " (('enter', 'three', 'witches'), 4),\n",
              " (('exeunt', 'scena', 'secunda'), 4),\n",
              " (('good', 'lord', 'macb'), 4),\n",
              " (('three', 'witches', '1'), 3),\n",
              " (('exeunt', 'scena', 'tertia'), 3),\n",
              " (('thunder', 'enter', 'three'), 3),\n",
              " (('exeunt', 'scena', 'quarta'), 3),\n",
              " (('scena', 'prima', 'enter'), 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Sử dụng văn bản trên mạng**"
      ],
      "metadata": {
        "id": "uQYDEQeBrzHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import thư viện và mở url để đọc file\n"
      ],
      "metadata": {
        "id": "lMs_skGwr8MX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib import request\n",
        "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode('utf8')\n",
        "raw[:75]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QjdQYZX1r9Sm",
        "outputId": "8c803e7e-378e-4f4f-81a5-180b5f53ea2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'*** START OF THE PROJECT GUTENBERG EBOOK 2554 ***\\n\\n\\n\\n\\nCRIME AND PUNISHMENT\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thay bằng các lệnh sau:\n"
      ],
      "metadata": {
        "id": "5l82yETXsCGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib import request\n",
        "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode('utf-8-sig')\n",
        "raw[:75]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "--k-Lpn5r1pp",
        "outputId": "9a4fee75-00d2-444b-990d-119dc63ce3fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'*** START OF THE PROJECT GUTENBERG EBOOK 2554 ***\\n\\n\\n\\n\\nCRIME AND PUNISHMENT\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thực hiện các lệnh sau:"
      ],
      "metadata": {
        "id": "bWQ24owRsMI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = nltk.word_tokenize (raw)\n",
        "webtext = nltk.Text (tokens)\n",
        "webtext[:12]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-J7HzIT9sNqV",
        "outputId": "d907db7b-dfbc-4955-cc7c-5f628279dbcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['*',\n",
              " '*',\n",
              " '*',\n",
              " 'START',\n",
              " 'OF',\n",
              " 'THE',\n",
              " 'PROJECT',\n",
              " 'GUTENBERG',\n",
              " 'EBOOK',\n",
              " '2554',\n",
              " '*',\n",
              " '*']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Rút trích văn bản từ trang html**"
      ],
      "metadata": {
        "id": "JTBt_vJUsT5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "cách rút trích văn\n",
        "bản từ trang html."
      ],
      "metadata": {
        "id": "iof8DDZVse7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
        "html = request.urlopen(url).read().decode('utf8')\n",
        "html[:120]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "zUlU7S0AsVsr",
        "outputId": "9c55515b-1768-48cf-8550-3afcba9db396"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN\" \"http://www.w3.org/TR/REC-html40/loose.dtd\">\\r\\n<html>\\r\\n<hea'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ta dùng thư viện bs4 (BeautifulSoup) cung cấp cho bạn các trình phân tích cú pháp phù hợp có\n",
        "thể nhận dạng HTML và trích xuất văn bản."
      ],
      "metadata": {
        "id": "GmQch3Kisn9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "raw = BeautifulSoup(html, \"lxml\").get_text()\n",
        "tokens = nltk.word_tokenize(raw)\n",
        "text = nltk.Text(tokens)\n"
      ],
      "metadata": {
        "id": "YMv6JVyAsoXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Phân tích cảm xúc người dùng**"
      ],
      "metadata": {
        "id": "Jlt-1aCnsqGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "download các movie review dùng lệnh sau:"
      ],
      "metadata": {
        "id": "-LwMqNPasvzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('movie_reviews')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRlIwms1srz9",
        "outputId": "3bc612f7-a7c2-4663-d9f2-a9b86a97c7e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sau đó xây dựng tập train dựa vào copus trên. Tạo 1 mảng tên là documents. Mảng này chứa cột\n",
        "đầu tiên là nội dung review của người dùng, và cột thứ 2 là cột đánh giá positive, neutral, or\n",
        "negative. Sau đó trộn các dòng này ngẫn nhiên"
      ],
      "metadata": {
        "id": "v7owb2ags2aQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "reviews = nltk.corpus.movie_reviews\n",
        "documents = [(list(reviews.words(fileid)), category)\n",
        "for category in reviews.categories()\n",
        "for fileid in reviews.fileids(category)]\n",
        "random.shuffle(documents)\n"
      ],
      "metadata": {
        "id": "nJdksG28s290"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Xem nội dung review đầu tiên (dòng 0, cột 0)\n"
      ],
      "metadata": {
        "id": "KGbFxgI8s8d4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_review = ' '.join(documents[0][0])\n",
        "print(first_review)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeefUBKOs-Ah",
        "outputId": "a8b107e4-a340-4af2-f37f-edf69714c1cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is not a simple plan about finding a plane load of money and getting away with the cash . this is more about a parable of greed , and how money can become the bane of your life . yes , there are elements of ? fargo ' here ( the snow and cold ) , although not as vivid in the blood and gore department . it shows how greed can set of a chain of events leading to death and the destruction of lives . and how at the end of the day , the things that matter most are love , truth and honesty . although in one sense it may seem tedious , making a movie about the ugliness of greed ( not box office material ) , it does become tedious , not because of the morale ending , but because one expects the movie to end that way . this becomes apparent after the first murder to cover up the crime , the rest of the movie just spirals downward from there . the characters in this drama are a mixture of simple and intellectual folk , brothers and friends , who all fall prey to the avarice of money . they should have perhaps thrown in someone sensible , level headed and not affected by greed to give the party more balance . commendable is the exceptional performance of billy bob thornton , whose portrayal of the simpleton brother was masterful . bill paxton also gives a powerful performance as the greedier younger brother , whilst bridget fonda is convincing as the greediest wife , who indirectly causes the most problems . fill a room full of greedy people and several million dollars , and you will end up with a simple recipe for a blood bath . it ' s not a simple plan , when you shoot everyone you love for money , unless you ' re the menendez brothers .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Xem kết quả review đầu tiên (dòng 0, cột 1)\n"
      ],
      "metadata": {
        "id": "KrVxWR5CtFgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0][1]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mJn-zv0ftF1q",
        "outputId": "e67dae5f-949d-426f-bffb-36ef47bbb1f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'neg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tạo bảng phân phối tần số các từ trong copus, bảng này cần chuyển sang dạng list, ta dùng\n",
        "hàm list()"
      ],
      "metadata": {
        "id": "HDNGv3J9tHbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = nltk.FreqDist(w.lower() for w in reviews.words())\n",
        "word_features = list(all_words)\n"
      ],
      "metadata": {
        "id": "pkUIbt0ltKa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sau đó, bước tiếp theo là xác định một hàm để tính toán các đặc trưng, tức là những từ đủ quan\n",
        "trọng để thiết lập ý kiến của một review."
      ],
      "metadata": {
        "id": "BZceGATZtNNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def document_features(document, word_features):\n",
        " document_words = set(document)\n",
        " features = {}\n",
        " for word in word_features:\n",
        "  features['{}'.format(word)] = (word in document_words)\n",
        " return features\n"
      ],
      "metadata": {
        "id": "uZ7C1O3ktO5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Khi bạn định nghĩa hàm document_features(), bạn tạo 1 tập các documents\n"
      ],
      "metadata": {
        "id": "u5Z-Mo9-tUMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "featuresets = [(document_features(d,word_features), c) for (d,c) in documents]\n",
        "len(featuresets)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OBmqR1etUeD",
        "outputId": "35a0ae03-c2d9-4aad-b61c-7ff746d35f79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tạo tập train và tập test: 1500 dòng đầu dùng cho tập train và 500 dòng còn lại dùng cho tập test\n",
        "để đánh giá độ chính xác của mô hình."
      ],
      "metadata": {
        "id": "LGgwYP3WtYnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, test_set = featuresets[1500:], featuresets[:500]\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n"
      ],
      "metadata": {
        "id": "IIZq5XHdtZEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dùng thuật toán Naïve Bayes để phân loại, dùng thư viện NLTK. Sau đó tính toán độ chính xác\n",
        "của thuật toán"
      ],
      "metadata": {
        "id": "lCtniKmmtcHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, est_set = featuresets[1500:], featuresets[:500]\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "print(nltk.classify.accuracy(classifier, test_set))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAYPXIeftcvP",
        "outputId": "de5bf3cf-ec40-427e-fe6d-93a5310b9972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chúng ta đã hoàn tất việc phân tích, dưới đây các từ với trọng số lớn nhất của các review được\n",
        "đánh giá là positive và negative\n"
      ],
      "metadata": {
        "id": "ieC47pjgvg3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.show_most_informative_features(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiNgLqkBvhNG",
        "outputId": "3f72c0ce-9053-4f72-9ef1-93a5e6f1b2ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Informative Features\n",
            "                captures = True              pos : neg    =      9.4 : 1.0\n",
            "              ridiculous = True              neg : pos    =      8.9 : 1.0\n",
            "               excellent = True              pos : neg    =      8.7 : 1.0\n",
            "              remarkable = True              pos : neg    =      8.7 : 1.0\n",
            "                 italian = True              pos : neg    =      8.0 : 1.0\n",
            "             magnificent = True              pos : neg    =      8.0 : 1.0\n",
            "                ordinary = True              pos : neg    =      8.0 : 1.0\n",
            "               spielberg = True              pos : neg    =      8.0 : 1.0\n",
            "                 laughed = True              neg : pos    =      7.9 : 1.0\n",
            "               painfully = True              neg : pos    =      7.9 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Bài tập áp dụng**"
      ],
      "metadata": {
        "id": "kuEfzyoLvy5H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.1 Viết chương trình Python với thư viện NLTK để liệt kê các tên của copus.**"
      ],
      "metadata": {
        "id": "qmco6gX-v2kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9rtFjvLpv11w",
        "outputId": "16ddcf73-4c8a-4a29-c297-c2b050b1040b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tải dữ liệu cần thiết\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('gutenberg')\n",
        "\n",
        "def extract_names(text):\n",
        "    tokens = word_tokenize(text)  # Tách từ\n",
        "    tagged_words = pos_tag(tokens)  # Gán nhãn từ loại\n",
        "    names = {word for word, tag in tagged_words if tag in ['NNP', 'NNPS']}  # Lọc danh từ riêng\n",
        "    return names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wG_4zlE8ySeZ",
        "outputId": "56891871-6ab6-40ed-a54c-3ee4d84a2570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Kiểm tra lại danh sách văn bản trong Gutenberg để đảm bảo có file\n",
        "print(\"Danh sách file trong Gutenberg:\", gutenberg.fileids())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6Kbe-2RyjEP",
        "outputId": "a77e25d4-b4c6-4743-e52e-5b380b3972a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Danh sách file trong Gutenberg: ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chọn văn bản (đảm bảo nó tồn tại)\n",
        "text = gutenberg.raw('carroll-alice.txt')\n",
        "names_list = extract_names(text)"
      ],
      "metadata": {
        "id": "On6k79njyk_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# liệt kê các tên của copus\n",
        "print(\"Danh sách tên riêng trong corpus:\")\n",
        "print(names_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoC6b-Inymcq",
        "outputId": "b4580ae9-b210-41c2-f0a1-0076af64d40a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Danh sách tên riêng trong corpus:\n",
            "{'WATCH', 'So', 'D', 'THEIR', 'Rabbit', 'Tears', ']', 'Elsie', 'France', 'FUL', \"'Come\", 'SOME', 'NEAR', 'Stigand', 'WILLIAM', 'Off', 'Dinn', 'HIS', 'BOOTS', 'Hare', 'Shark', 'COULD', 'Please', 'Poor', 'NOT', 'Ada', 'Footman', \"'It\", 'Turtle', 'Between', 'Mary', 'England', 'Arithmetic', 'MINE', 'BEST', 'Lobster', 'Lory', 'DOTH', \"'poison\", 'HERE', 'O', 'WITH', 'Mock', 'Queen', 'Grammar', 'Longitude', 'Dodo', 'Pig', 'WHATEVER', 'THIS', 'Heads', 'Serpent', 'Owl', 'Five', 'Soo', 'PRECIOUS', \"'You\", 'Pepper', 'A', 'Will', 'HEARTHRUG', \"'There\", 'SHE', 'Advice', 'Take', 'XI', 'AT', 'Adventures', 'Caucus-race', 'OUTSIDE', 'Be', 'ITS', 'Quick', 'II', 'Ma', 'SHOES', 'Table', 'Duchess', 'PLEASE', 'Brandy', 'HAVE', 'IV', 'Time', 'Has', 'Canary', 'Lizard', 'INSIDE', 'Tillie', 'One', 'Run', 'William', 'BUSY', 'Ah', 'Gryphon.', 'London', 'Pray', 'Puss', \"'it\", 'THESE', 'KING', 'W', 'HIGH', 'Turn', 'HEARTS', 'Stretching', 'Pennyworth', 'FROM', 'Bill', 'WOULD', 'YOUR', 'Evidence', 'Paris', 'Coils', 'Latitude', 'BE', 'FATHER', 'HER', 'MORE', \"'how\", 'Nay', 'New', 'Sends', 'Never', 'NO', 'Gryphon', 'FIT', 'CHAPTER', 'Father', 'Coming', \"'you\", 'Come', 'VIII', 'White', 'French', \"'they\", 'Seven', 'V.', 'Soon', 'Hearts', 'Down', 'Tarts', 'First', 'OURS', 'THERE', 'TO', 'GAVE', 'Quadrille', 'Suppress', 'AND', 'BEE', 'Beautiful', 'PERSONS', 'Silence', 'Tortoise', 'Fish-Footman', 'ALICE', 'X', 'Ann', 'Which', 'LOVE', 'Forty-two', 'OLD', 'Visit', 'Australia', 'THE', 'WAS', 'Lewis', 'ME', 'Edwin', 'Ambition', 'Canterbury', 'English', 'Story', 'FOOT', 'WHAT', 'March', 'RIGHT', 'Tis', 'Nile', 'Latin', 'Drawling', \"Ma'am\", 'Dinah', 'Mercia', \"'Curiouser\", 'Lacie', 'ARE', 'Duck', 'LEAVE', 'TWO', 'Magpie', 'Writhing', 'COURT', 'Cat', 'Pool', 'ALL', 'Trims', 'Soup', 'VII', 'SOMEBODY', 'Queens', 'Edgar', 'Ugh', 'MYSELF', 'RABBIT', 'Beau', 'Mouse', 'IN', 'YOU.', 'SOUP', 'Distraction', 'Rome', 'VI', 'IF', 'KNOW', 'SAID', 'Imagine', 'Crab', 'Jack-in-the-box', 'Caterpillar', 'Miss', 'Sounds', 'Fainting', 'IT', 'HATED', 'ONE', 'Mabel', 'Northumbria', 'Said', 'Cheshire', 'Panther', 'Alice', 'MARMALADE', 'THAT', 'Pat', 'Drawling-master', 'May', \"'and\", 'Eaglet', 'NEVER', 'Good-bye', 'Majesty', 'CHORUS', 'means', 'Conqueror', 'Uglification', 'Pigeon', 'Twinkle', 'King', 'FENDER', 'Wonderland', 'WAISTCOAT-POCKET', 'III', 'Half-past', '_I_', \"'And\", \"'arrum\", \"'moral\", 'Him', 'HOW', 'Sir', 'THEN', 'LITTLE', 'Classics', 'Number', 'HIM', 'THAN', 'QUEEN', '*', 'Christmas', 'Carroll', 'MUST', 'Derision', 'Frog-Footman', 'Keep', 'SWIM', 'Normans', \"'The\", 'PLENTY', 'VERY', 'TRUE', 'Do', 'Hatter', 'Atheling', 'Seaography', 'Antipathies', 'XII', 'HE', 'Geography', \"'This\", 'MILE', 'Shakespeare', 'WASHING', 'Knave', 'OF', 'Dormouse', 'Was', 'VOICE', 'RETURNED', 'Game', 'Morcar', 'CAN', 'Prizes', 'SLUGGARD', 'Multiplication', 'Mystery', 'Cat.', 'IX', 'Kings', 'OUT', 'YOU', 'How', 'M', 'Grief', 'Zealand', 'Too', 'C', 'ESQ', 'THEY', 'WE', \"'Turn\", 'Laughing'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.2 Viết chương trình Python với thư viện NLTK để liệt kê danh sách các stopword bằng các ngôn ngữ khác nhau**"
      ],
      "metadata": {
        "id": "jOHk6ifkzAEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "languages = stopwords.fileids()\n",
        "for lang in languages:\n",
        "    print(f\"Stopwords ({lang}): {stopwords.words(lang)[:10]} ...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUgaiBI_zJTg",
        "outputId": "b8a352cc-770a-4829-e49b-d68c6cc665d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopwords (albanian): ['tyre', 'rreth', 'le', 'atyre', 'këta', 'megjithëse', 'kemi', 'per', 'ndonëse', 'dytë'] ...\n",
            "Stopwords (arabic): ['إذ', 'إذا', 'إذما', 'إذن', 'أف', 'أقل', 'أكثر', 'ألا', 'إلا', 'التي'] ...\n",
            "Stopwords (azerbaijani): ['a', 'ad', 'altı', 'altmış', 'amma', 'arasında', 'artıq', 'ay', 'az', 'bax'] ...\n",
            "Stopwords (basque): ['ahala', 'aitzitik', 'al', 'ala ', 'alabadere', 'alabaina', 'alabaina', 'aldiz ', 'alta', 'amaitu'] ...\n",
            "Stopwords (belarusian): ['на', 'не', 'што', 'па', 'да', 'за', 'як', 'для', 'гэта', 'ад'] ...\n",
            "Stopwords (bengali): ['অতএব', 'অথচ', 'অথবা', 'অনুযায়ী', 'অনেক', 'অনেকে', 'অনেকেই', 'অন্তত', 'অন্য', 'অবধি'] ...\n",
            "Stopwords (catalan): ['a', 'abans', 'ací', 'ah', 'així', 'això', 'al', 'aleshores', 'algun', 'alguna'] ...\n",
            "Stopwords (chinese): ['一', '一下', '一些', '一切', '一则', '一天', '一定', '一方面', '一旦', '一时'] ...\n",
            "Stopwords (danish): ['og', 'i', 'jeg', 'det', 'at', 'en', 'den', 'til', 'er', 'som'] ...\n",
            "Stopwords (dutch): ['de', 'en', 'van', 'ik', 'te', 'dat', 'die', 'in', 'een', 'hij'] ...\n",
            "Stopwords (english): ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an'] ...\n",
            "Stopwords (finnish): ['olla', 'olen', 'olet', 'on', 'olemme', 'olette', 'ovat', 'ole', 'oli', 'olisi'] ...\n",
            "Stopwords (french): ['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle'] ...\n",
            "Stopwords (german): ['aber', 'alle', 'allem', 'allen', 'aller', 'alles', 'als', 'also', 'am', 'an'] ...\n",
            "Stopwords (greek): ['αλλα', 'αν', 'αντι', 'απο', 'αυτα', 'αυτεσ', 'αυτη', 'αυτο', 'αυτοι', 'αυτοσ'] ...\n",
            "Stopwords (hebrew): ['אני', 'את', 'אתה', 'אנחנו', 'אתן', 'אתם', 'הם', 'הן', 'היא', 'הוא'] ...\n",
            "Stopwords (hinglish): ['a', 'aadi', 'aaj', 'aap', 'aapne', 'aata', 'aati', 'aaya', 'aaye', 'ab'] ...\n",
            "Stopwords (hungarian): ['a', 'ahogy', 'ahol', 'aki', 'akik', 'akkor', 'alatt', 'által', 'általában', 'amely'] ...\n",
            "Stopwords (indonesian): ['ada', 'adalah', 'adanya', 'adapun', 'agak', 'agaknya', 'agar', 'akan', 'akankah', 'akhir'] ...\n",
            "Stopwords (italian): ['ad', 'al', 'allo', 'ai', 'agli', 'all', 'agl', 'alla', 'alle', 'con'] ...\n",
            "Stopwords (kazakh): ['ах', 'ох', 'эх', 'ай', 'эй', 'ой', 'тағы', 'тағыда', 'әрине', 'жоқ'] ...\n",
            "Stopwords (nepali): ['छ', 'र', 'पनि', 'छन्', 'लागि', 'भएको', 'गरेको', 'भने', 'गर्न', 'गर्ने'] ...\n",
            "Stopwords (norwegian): ['og', 'i', 'jeg', 'det', 'at', 'en', 'et', 'den', 'til', 'er'] ...\n",
            "Stopwords (portuguese): ['a', 'à', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as'] ...\n",
            "Stopwords (romanian): ['a', 'abia', 'acea', 'aceasta', 'această', 'aceea', 'aceeasi', 'acei', 'aceia', 'acel'] ...\n",
            "Stopwords (russian): ['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со'] ...\n",
            "Stopwords (slovene): ['ali', 'ampak', 'bodisi', 'in', 'kajti', 'marveč', 'namreč', 'ne', 'niti', 'oziroma'] ...\n",
            "Stopwords (spanish): ['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se'] ...\n",
            "Stopwords (swedish): ['och', 'det', 'att', 'i', 'en', 'jag', 'hon', 'som', 'han', 'på'] ...\n",
            "Stopwords (tajik): ['аз', 'дар', 'ба', 'бо', 'барои', 'бе', 'то', 'ҷуз', 'пеши', 'назди'] ...\n",
            "Stopwords (turkish): ['acaba', 'ama', 'aslında', 'az', 'bazı', 'belki', 'biri', 'birkaç', 'birşey', 'biz'] ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.3. Viết chương trình Python với thư viện NLTK để kiểm tra danh sách các stopword bằng các ngôn ngữ khác nhau.**"
      ],
      "metadata": {
        "id": "83vbrZO9zRpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_stopword(word, lang):\n",
        "    return word.lower() in stopwords.words(lang)\n",
        "\n",
        "print(\"the in english: \",check_stopword(\"the\", \"english\"))  # True\n",
        "print(\"le in french: \",check_stopword(\"le\", \"french\"))    # True\n",
        "print(\"tyre in english: \",check_stopword(\"tyre\", \"english\"))   # False\n",
        "print(\"det in danish: \",check_stopword(\"det\", \"danish\"))    # True\n",
        "print(\"amma in english: \",check_stopword(\"amma\", \"english\"))   # False\n",
        "print(\"অতএব in nepali: \", check_stopword(\"অতএব\", \"nepali\"))   # False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EB9ImmOxzV9F",
        "outputId": "f053f97c-bd94-4421-f17b-fa4a2f348d56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the in english:  True\n",
            "le in french:  True\n",
            "tyre in english:  False\n",
            "det in danish:  True\n",
            "amma in english:  False\n",
            "অতএব in nepali:  False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.4 Viết chương trình Python với thư viện NLTK để loại bỏ các stopword từ một văn bản đã cho**"
      ],
      "metadata": {
        "id": "Y5VZjCpQ1RFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text, lang=\"english\"):\n",
        "    words = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words(lang))\n",
        "    return \" \".join([word for word in words if word.lower() not in stop_words])\n",
        "\n",
        "sample_text = \"This is an example sentence demonstrating stopword removal.\"\n",
        "print(remove_stopwords(sample_text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsCaV2vZ1TNY",
        "outputId": "f5578c6a-20fd-463c-a29f-b54f9dfcd271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example sentence demonstrating stopword removal .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.5 Viết chương trình Python với thư viện NLTK bỏ qua các stopword từ danh sách các stopword.**"
      ],
      "metadata": {
        "id": "H-LLkPU41qAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_stopwords = set(stopwords.words(\"english\")) - {\"not\", \"no\"}\n",
        "print(custom_stopwords)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcnQZlJk1v4x",
        "outputId": "c7bf5025-b142-4c37-a76b-03c1e08315e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'up', 'couldn', \"don't\", 'will', 'because', 'if', 'don', \"we'd\", 'after', 'but', 'of', \"you'll\", 'at', \"we're\", \"i'm\", 'mightn', 'didn', \"it's\", \"i'll\", 'hers', 'were', 'am', 'shouldn', 'hadn', \"shouldn't\", 'other', 'each', 'further', 'have', 'your', 'm', \"she's\", 'now', 'some', 'herself', \"that'll\", \"won't\", 'same', 'both', 'own', \"hadn't\", 'we', 'against', 'i', 'mustn', \"mightn't\", 'doing', 'me', 'should', 'yours', 'how', 'isn', 'only', 'wouldn', 'he', 'been', \"they'll\", 'aren', 'off', 'their', 'so', \"we've\", 'has', 'wasn', 'what', 'who', 'ourselves', 'or', 'under', 'haven', \"mustn't\", 'out', 'with', 'through', 'll', \"she'd\", 'whom', 'are', 'ain', 'about', 'being', 'its', 'on', 'him', 'such', \"he's\", 'and', \"they're\", 'y', 'just', 't', 'doesn', 'during', 'you', 'having', 'these', 'by', \"she'll\", 'a', \"you'd\", \"i'd\", \"he'd\", 'do', 'hasn', 'his', 'for', 'few', 'between', 'can', 'most', 've', 'which', 'those', \"they've\", 'then', 'the', \"shan't\", 'himself', 'while', 'before', 'again', 'as', 'yourselves', 'down', \"haven't\", 'd', 'this', 'very', 'there', \"he'll\", 'an', 'won', \"hasn't\", 'all', 'any', 'in', 'o', 'once', 'does', \"doesn't\", 're', 'until', 'needn', \"didn't\", 'weren', 'be', 'more', 'than', 'yourself', 'our', 'ours', \"we'll\", \"weren't\", 'to', 'when', 'itself', 'into', \"you're\", 'was', 's', 'had', 'my', 'she', 'nor', \"should've\", \"needn't\", 'above', 'ma', 'theirs', 'it', \"wasn't\", 'they', \"you've\", 'her', 'them', \"they'd\", \"wouldn't\", \"i've\", 'below', \"it'll\", 'from', 'that', 'where', \"it'd\", 'did', 'myself', 'is', 'too', 'over', 'shan', 'why', \"couldn't\", 'here', \"aren't\", \"isn't\", 'themselves'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.6 Viết một chương trình Python với thư viện NLTK để tìm định nghĩa và ví dụ của một từ đã\n",
        "cho bằng WordNet từ Wikipedia**"
      ],
      "metadata": {
        "id": "9xXpeIRY1xlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def get_word_info(word):\n",
        "    synsets = wordnet.synsets(word)\n",
        "    for syn in synsets:\n",
        "        print(f\"Định nghĩa: {syn.definition()}\")\n",
        "        print(f\"Ví dụ: {syn.examples()}\")\n",
        "\n",
        "get_word_info(\"computer\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4b9Hl8j11la",
        "outputId": "4d10d0a5-54bb-4225-ead4-5f13a4ade102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Định nghĩa: a machine for performing calculations automatically\n",
            "Ví dụ: []\n",
            "Định nghĩa: an expert at calculation (or at operating calculating machines)\n",
            "Ví dụ: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.7. Viết chương trình Python với thư viện NLTK để tìm tập hợp các từ đồng nghĩa và trái nghĩa\n",
        "của một từ nào đó.**"
      ],
      "metadata": {
        "id": "l6ABa30z1-Eb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def synonyms_antonyms(word):\n",
        "    synonyms, antonyms = set(), set()\n",
        "\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name())\n",
        "            if lemma.antonyms():\n",
        "                antonyms.add(lemma.antonyms()[0].name())\n",
        "\n",
        "    print(f\"Từ đồng nghĩa của happy: {synonyms}\")\n",
        "    print(f\"Từ trái nghĩa của happy: {antonyms}\")\n",
        "\n",
        "synonyms_antonyms(\"happy\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXmCBlNl1_qu",
        "outputId": "5b6d06b3-9f85-4ccd-ec6d-c9bd53017174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Từ đồng nghĩa của happy: {'well-chosen', 'felicitous', 'glad', 'happy'}\n",
            "Từ trái nghĩa của happy: {'unhappy'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.8 Viết chương trình Python với thư viện NLTK để có cái nhìn tổng quan về bộ tag, chi tiết của\n",
        "một tag cụ thể trong bộ tag và chi tiết về một số bộ tag liên quan, sử dụng biểu thức chính\n",
        "quy**"
      ],
      "metadata": {
        "id": "ZtkYrXpY2L0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('tagsets')\n",
        "\n",
        "nltk.help.upenn_tagset('NN')  # Xem chi tiết tag danh từ\n",
        "nltk.help.upenn_tagset()       # Xem toàn bộ tag\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbuK4oqn2QNS",
        "outputId": "c8602e47-1e0a-48ca-e21e-7097ec388e10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN: noun, common, singular or mass\n",
            "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
            "    investment slide humour falloff slick wind hyena override subhumanity\n",
            "    machinist ...\n",
            "$: dollar\n",
            "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
            "'': closing quotation mark\n",
            "    ' ''\n",
            "(: opening parenthesis\n",
            "    ( [ {\n",
            "): closing parenthesis\n",
            "    ) ] }\n",
            ",: comma\n",
            "    ,\n",
            "--: dash\n",
            "    --\n",
            ".: sentence terminator\n",
            "    . ! ?\n",
            ":: colon or ellipsis\n",
            "    : ; ...\n",
            "CC: conjunction, coordinating\n",
            "    & 'n and both but either et for less minus neither nor or plus so\n",
            "    therefore times v. versus vs. whether yet\n",
            "CD: numeral, cardinal\n",
            "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
            "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
            "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
            "DT: determiner\n",
            "    all an another any both del each either every half la many much nary\n",
            "    neither no some such that the them these this those\n",
            "EX: existential there\n",
            "    there\n",
            "FW: foreign word\n",
            "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
            "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
            "    terram fiche oui corporis ...\n",
            "IN: preposition or conjunction, subordinating\n",
            "    astride among uppon whether out inside pro despite on by throughout\n",
            "    below within for towards near behind atop around if like until below\n",
            "    next into if beside ...\n",
            "JJ: adjective or numeral, ordinal\n",
            "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
            "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
            "    multilingual multi-disciplinary ...\n",
            "JJR: adjective, comparative\n",
            "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
            "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
            "    cozier creamier crunchier cuter ...\n",
            "JJS: adjective, superlative\n",
            "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
            "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
            "    dearest deepest densest dinkiest ...\n",
            "LS: list item marker\n",
            "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
            "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
            "    two\n",
            "MD: modal auxiliary\n",
            "    can cannot could couldn't dare may might must need ought shall should\n",
            "    shouldn't will would\n",
            "NN: noun, common, singular or mass\n",
            "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
            "    investment slide humour falloff slick wind hyena override subhumanity\n",
            "    machinist ...\n",
            "NNP: noun, proper, singular\n",
            "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
            "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
            "    Shannon A.K.C. Meltex Liverpool ...\n",
            "NNPS: noun, proper, plural\n",
            "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
            "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
            "    Apache Apaches Apocrypha ...\n",
            "NNS: noun, common, plural\n",
            "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
            "    divestitures storehouses designs clubs fragrances averages\n",
            "    subjectivists apprehensions muses factory-jobs ...\n",
            "PDT: pre-determiner\n",
            "    all both half many quite such sure this\n",
            "POS: genitive marker\n",
            "    ' 's\n",
            "PRP: pronoun, personal\n",
            "    hers herself him himself hisself it itself me myself one oneself ours\n",
            "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
            "PRP$: pronoun, possessive\n",
            "    her his mine my our ours their thy your\n",
            "RB: adverb\n",
            "    occasionally unabatingly maddeningly adventurously professedly\n",
            "    stirringly prominently technologically magisterially predominately\n",
            "    swiftly fiscally pitilessly ...\n",
            "RBR: adverb, comparative\n",
            "    further gloomier grander graver greater grimmer harder harsher\n",
            "    healthier heavier higher however larger later leaner lengthier less-\n",
            "    perfectly lesser lonelier longer louder lower more ...\n",
            "RBS: adverb, superlative\n",
            "    best biggest bluntest earliest farthest first furthest hardest\n",
            "    heartiest highest largest least less most nearest second tightest worst\n",
            "RP: particle\n",
            "    aboard about across along apart around aside at away back before behind\n",
            "    by crop down ever fast for forth from go high i.e. in into just later\n",
            "    low more off on open out over per pie raising start teeth that through\n",
            "    under unto up up-pp upon whole with you\n",
            "SYM: symbol\n",
            "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
            "TO: \"to\" as preposition or infinitive marker\n",
            "    to\n",
            "UH: interjection\n",
            "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
            "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
            "    man baby diddle hush sonuvabitch ...\n",
            "VB: verb, base form\n",
            "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
            "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
            "    boost brace break bring broil brush build ...\n",
            "VBD: verb, past tense\n",
            "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
            "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
            "    speculated wore appreciated contemplated ...\n",
            "VBG: verb, present participle or gerund\n",
            "    telegraphing stirring focusing angering judging stalling lactating\n",
            "    hankerin' alleging veering capping approaching traveling besieging\n",
            "    encrypting interrupting erasing wincing ...\n",
            "VBN: verb, past participle\n",
            "    multihulled dilapidated aerosolized chaired languished panelized used\n",
            "    experimented flourished imitated reunifed factored condensed sheared\n",
            "    unsettled primed dubbed desired ...\n",
            "VBP: verb, present tense, not 3rd person singular\n",
            "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
            "    appear tend stray glisten obtain comprise detest tease attract\n",
            "    emphasize mold postpone sever return wag ...\n",
            "VBZ: verb, present tense, 3rd person singular\n",
            "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
            "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
            "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
            "WDT: WH-determiner\n",
            "    that what whatever which whichever\n",
            "WP: WH-pronoun\n",
            "    that what whatever whatsoever which who whom whosoever\n",
            "WP$: WH-pronoun, possessive\n",
            "    whose\n",
            "WRB: Wh-adverb\n",
            "    how however whence whenever where whereby whereever wherein whereof why\n",
            "``: opening quotation mark\n",
            "    ` ``\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.9 Viết chương trình Python với thư viện NLTK để so sánh sự giống nhau của hai danh từ đã cho.**"
      ],
      "metadata": {
        "id": "7iHns7g-2Vzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word1 = wordnet.synsets('cat', pos=wordnet.NOUN)[0]\n",
        "word2 = wordnet.synsets('dog', pos=wordnet.NOUN)[0]\n",
        "\n",
        "print(\"Mức độ giống nhau giữa cat và dog:\", word1.wup_similarity(word2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaBOo1Ks2diU",
        "outputId": "04ab5ca5-8011-4efd-ccc3-a15a81b1e7c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mức độ giống nhau giữa cat và dog: 0.8571428571428571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.10 Viết chương trình Python với thư viện NLTK để so sánh sự giống nhau của hai động từ đã\n",
        "cho**"
      ],
      "metadata": {
        "id": "t2WC6Q-P2iJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word1 = wordnet.synsets('run', pos=wordnet.VERB)[0]\n",
        "word2 = wordnet.synsets('walk', pos=wordnet.VERB)[0]\n",
        "\n",
        "print(\"Mức độ giống nhau giữa run và walk:\", word1.wup_similarity(word2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctMXGLYo2mwP",
        "outputId": "a259823f-4c3f-4155-ca8f-152dbc931cba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mức độ giống nhau giữa run và walk: 0.2857142857142857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.11 Viết chương trình Python với thư viện NLTK để tìm số lượng tên nam và nữ trong các tên\n",
        "kho ngữ liệu. In tên 10 nam và nữ đầu tiên. Lưu ý: Kho văn bản tên chứa tổng cộng khoảng\n",
        "2943 nam (male.txt) và 5001 nữ (Female.txt) tên. Kho được biên soạn bởi Kantrowitz, Ross**"
      ],
      "metadata": {
        "id": "e1twaoJk4Cy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import names\n",
        "\n",
        "nltk.download('names')\n",
        "\n",
        "male_names = names.words('male.txt')\n",
        "female_names = names.words('female.txt')\n",
        "\n",
        "print(f\"Số lượng tên nam: {len(male_names)}\")\n",
        "print(f\"Số lượng tên nữ: {len(female_names)}\")\n",
        "\n",
        "print(\"10 tên nam đầu tiên:\", male_names[:10])\n",
        "print(\"10 tên nữ đầu tiên:\", female_names[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cchtrJmI4GFy",
        "outputId": "e1a8cebf-10b0-448f-e8eb-cddbd0524331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Số lượng tên nam: 2943\n",
            "Số lượng tên nữ: 5001\n",
            "10 tên nam đầu tiên: ['Aamir', 'Aaron', 'Abbey', 'Abbie', 'Abbot', 'Abbott', 'Abby', 'Abdel', 'Abdul', 'Abdulkarim']\n",
            "10 tên nữ đầu tiên: ['Abagael', 'Abagail', 'Abbe', 'Abbey', 'Abbi', 'Abbie', 'Abby', 'Abigael', 'Abigail', 'Abigale']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.12 Viết chương trình Python với thư viện NLTK để in 15 kết hợp ngẫu nhiên đầu tiên được gắn\n",
        "nhãn nam và được gắn nhãn tên nữ từ kho tên.**"
      ],
      "metadata": {
        "id": "UhAtj8o14I4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "random_male = random.sample(male_names, 15)\n",
        "random_female = random.sample(female_names, 15)\n",
        "\n",
        "print(\"Tên nam:\", random_male)\n",
        "print(\"Tên nữ:\", random_female)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_SXQg974LC-",
        "outputId": "47db430a-481f-4a34-ef26-89595860672f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tên nam: ['Yancey', 'Sonny', 'Thain', 'Armond', 'Vinnie', 'Winifield', 'Jacques', 'Stewart', 'Johan', 'Barr', 'Sayre', 'Ozzie', 'Lion', 'Rodrique', 'Marcellus']\n",
            "Tên nữ: ['Luciana', 'Marylinda', 'Kiri', 'Hellene', 'Leland', 'Ailey', 'Brande', 'Janeen', 'Karly', 'Tallie', 'Eve', 'Trina', 'Dela', 'Cicely', 'Deerdre']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.13 Viết chương trình Python với thư viện NLTK để trích xuất ký tự cuối cùng của tất cả các tên\n",
        "được gắn nhãn và tạo mảng mới với chữ cái cuối cùng của mỗi tên và nhãn được liên kết.**"
      ],
      "metadata": {
        "id": "T7O9AUPy4Pxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_last_letter():\n",
        "    labeled_names = [(name[-1], 'male') for name in male_names] + [(name[-1], 'female') for name in female_names]\n",
        "    return labeled_names\n",
        "\n",
        "print(\"Mẫu ký tự cuối cùng và nhãn:\")\n",
        "print(extract_last_letter()[:15])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxl0Quy64Xwy",
        "outputId": "2c1bbb5c-1f65-4470-fc56-81d32cb0a17f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mẫu ký tự cuối cùng và nhãn:\n",
            "[('r', 'male'), ('n', 'male'), ('y', 'male'), ('e', 'male'), ('t', 'male'), ('t', 'male'), ('y', 'male'), ('l', 'male'), ('l', 'male'), ('m', 'male'), ('h', 'male'), ('e', 'male'), ('l', 'male'), ('d', 'male'), ('r', 'male')]\n"
          ]
        }
      ]
    }
  ]
}